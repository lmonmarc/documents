%% -*- eval: (flyspell-mode 1); -*-

\chapter{Introduction}
%\addcontentsline{toc}{chapter}{Introduction}

Un des aspects de la recherche informatique consiste à améliorer des programmes afin de rendre leur temps d'exécution plus rapide, il s'agit du \emph{calcul haute performance} (ou HPC, pour High Performance Computing). Les performances -- en matière de temps d'exécution -- sont primordiales par exemple pour les codes de simulation numérique dédiés à la météorologie qui calculent les prévisions du lendemain, où bien entendu la simulation doit donc s'être terminée en moins d'une nuit. Dans ce cadre le stage a porté sur l'obtention automatique de certains de ces programmes améliorés.


\section{Parallélisation de programmes}

Diverses techniques existent pour améliorer les performances d'un programme et plus particulièrement de son temps d'exécution. Deux aspects peuvent entrer en compte : les algorithmes utilisés dans le programme ainsi que le matériel qui l'exécute. Le facteur limitant à améliorer en priorité est alors le matériel, car il influe sur la nature des algorithmes pouvant être utilisés.

La solution la plus simple consisterait à se baser uniquement sur la loi de Moore, c'est-à-dire sur le fait que les capacités du matériel sur lequel le programme est exécuté sont doublées tous les ans. Or cette loi exponentielle n'est maintenant plus tout-à-fait valable puisque par exemple la fréquence d'horloge des processeurs modernes stagne depuis quelques années à une valeur proche de $3$ GHz. C'est cette fréquence d'horloge qui impose au matériel (si celui-ci est constitué de cet unique processeur) le nombre d'opérations qu'il va pouvoir effectuer en une seconde. Puisque la fréquence n'augmente plus, le nombre d'opérations par seconde pour ce type de matériel ne peut plus augmenter non plus, aucun gain de rapidité d'exécution n'est alors possible.

La première solution s'appuyait ainsi sur l'augmentation du nombre d'opérations par seconde supporté par le matériel, possédant \emph{un seul} processeur. Une autre solution est donc d'augmenter le nombre de processeurs, il s'agit de l'introduction du parallélisme dans le matériel. Celui-ci peut s'effectuer de plusieurs manières : à l'intérieur et à l'extérieur du processeur. À l'intérieur, il s'agit soit de subdiviser les instructions en  micro-instructions afin de créer un pipeline (ainsi une instruction peut commencer immédiatement après la fin de la première micro-instruction de l'instruction précédente), soit de dupliquer certains composants de l'architecture afin d'exécuter la même instruction sur plusieurs données à la fois. Les processeurs modernes intègrent tous ces deux technologies : le pipeline d'instructions et l'architecture nommée superscalaire.  À l'extérieur il s'agit de connecter plusieurs processeurs les uns avec les autres, tous participant aux calculs nécessités par le programme de base. Différentes manières de relier les processeurs existent, ils peuvent alors être appelés des cœurs ; dans toute la suite du rapport, un processeur est synonyme d'un ensemble de cœurs connectés entre eux par de la mémoire cache hiérarchique en accès uniforme\footnote{Cette notion sera explicitée au point \ldots~ .}.

En modifiant la nature du matériel (et non seulement ses capacités d'ordre physique comme la fréquence d'horloge) cela induit des modifications dans la façon de transcrire les algorithmes (par exemple pour utiliser le côté superscalaire), ainsi que dans les algorithmes eux-mêmes qui ne sont parfois plus du tout adaptés (notamment à cause des communications entre les processeurs, inexistantes auparavant). Écrire un programme destiné à une machine parallèle nécessite donc des algorithmes souvent très spécifiques à la configuration de la machine d'exécution, ce qui est d'autant plus complexe avec les architectures des machines actuelles.

\section{\'Ecriture de code parallèle}

Les algorithmes étant souvent spécifiques au type de parallélisme et à l'architecture de la machine cible, les langages permettant d'écrire des programmes parallèles automatisent peu la génération de codes parfaitement adaptés afin d'obtenir l'optimum des performances. En pratique la plupart des langages modernes permettent l'expression du parallélisme sans le découvrir eux-même ; certains outils aident à écrire du code parallèle mais ils sont souvent expérimentaux et nécessitent alors que le code automatiquement produit soit vérifié.

Les difficultés inhérentes à la production d'un programme parallèle sont de plusieurs natures, notamment l'identification des sections parallèles et de celles obligatoirement séquentielles, ainsi que la gestion des communication et l'utilisation de toutes les ressources disponibles sur la matériel. L'identification des régions parallélisables ou non dépend de l'algorithme utilisé pour résoudre le problème voulu. Un programme bien parallélisé réduit au maximum le nombre d'opérations qui sont exécutées séquentiellement, afin que la majeure partie du temps d'exécution soit passée dans des sections parallèles où toute la puissance de la machine parallèle est utilisée. Par ailleurs il est essentiel de gérer les communications entre les différents processeurs afin que des calculs puissent être effectués pendant les communications ; on parle alors du recouvrement des communications. Enfin il convient d'utiliser le plus de ressources possibles sans que cette utilisation n'entraîne d'effort trop important, par exemple si de nombreuses parties de l'algorithme nécessitent une synchronisation de tous les processeurs -- ce qui est très coûteux en temps.

Actuellement c'est au programmeur qu'incombent presque toutes ces tâches, or cela nécessite beaucoup de compétences, de temps, et de tests avant de parvenir à un résultat acceptable. S'il n'est pas toujours possible de découvrir le parallélisme automatiquement, il n'est pas non plus toujours facile de savoir où le décrire. Alors que certaines options sont très proches du processeur (comme la caractère superscalaire) et doivent être gérées de manière assez fines, d'autres peuvent être prises en charge à un plus haut-niveau comme le lancement de plusieurs procédures distinctes en parallèle (a priori chacune des procédures étant alors associée à un processeur libre). Cela amène à une hiérarchisation du parallélisme, ce qui complique encore plus la tâche du programmeur.

\section{Choix du niveau d'expression du parallélisme}

Où décrire le parallèlisme ? Le trouver automatiquement ?
cf Qiral et Maude

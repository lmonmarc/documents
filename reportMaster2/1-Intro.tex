%% -*- eval: (flyspell-mode 1); -*-

\chapter{Introduction}
%\addcontentsline{toc}{chapter}{Introduction}

Un des aspects de la recherche informatique consiste à améliorer des programmes afin de rendre leur temps d'exécution plus rapide, il s'agit du \emph{calcul haute performance} (ou HPC, pour High Performance Computing). Les performances -- en matière de temps d'exécution -- sont primordiales par exemple pour les codes de simulation numérique dédiés à la météorologie qui calculent les prévisions du lendemain, où bien entendu la simulation doit donc s'être terminée en moins d'une nuit. Dans ce cadre le stage a porté sur l'obtention automatique de certains de ces programmes améliorés.


\section{Parallélisation de programmes}

Diverses techniques existent pour améliorer les performances d'un programme et plus particulièrement de son temps d'exécution. Deux aspects peuvent entrer en compte : les algorithmes utilisés dans le programme ainsi que le matériel qui l'exécute. Le facteur limitant à améliorer en priorité est alors le matériel, car il influe sur la nature des algorithmes pouvant être utilisés.

La solution la plus simple consisterait à se baser uniquement sur la loi de Moore, c'est-à-dire sur le fait que les capacités du matériel sur lequel le programme est exécuté sont doublées tous les ans. Or cette loi exponentielle n'est maintenant plus tout-à-fait valable puisque par exemple la fréquence d'horloge des processeurs modernes stagne depuis quelques années à une valeur proche de $3$ GHz. C'est cette fréquence d'horloge qui impose au matériel (si celui-ci est constitué de cet unique processeur) le nombre d'opérations qu'il va pouvoir effectuer en une seconde. Puisque la fréquence n'augmente plus, le nombre d'opérations par seconde pour ce type de matériel ne peut plus augmenter non plus, aucun gain de rapidité d'exécution n'est alors possible.

La première solution s'appuyait ainsi sur l'augmentation de la fréquence, c'est-à-dire de la rapidité de chaque opération sachant qu'une seule est faite à la fois. Au lieu d'augmenter la rapidité des opérations, une autre solution est donc d'en augmenter le nombre effectuées \emph{en même temps}, il s'agit de l'introduction du parallélisme dans le matériel. Celui-ci peut s'effectuer de plusieurs manières : à l'intérieur et à l'extérieur du processeur. À l'intérieur, il s'agit soit de subdiviser les instructions en  micro-instructions afin de créer un pipeline (ainsi une instruction peut commencer immédiatement après la fin de la première micro-instruction de l'instruction précédente), soit de dupliquer certains composants de l'architecture afin d'exécuter la même instruction sur plusieurs données à la fois. Les processeurs modernes intègrent tous ces deux technologies : le pipeline d'instructions et l'architecture nommée \emph{superscalaire}.  À l'extérieur il s'agit de connecter plusieurs processeurs les uns avec les autres, tous participant aux calculs nécessités par le programme de base. Différentes manières de relier les processeurs existent, ils peuvent alors être appelés des cœurs ; dans toute la suite du rapport, un processeur est synonyme d'un ensemble de cœurs connectés entre eux par de la mémoire cache hiérarchique en accès uniforme\footnote{Cette notion sera explicitée au point \ref{sec:stencil_base}.}. Enfin il convient des différencier les processeurs classiques -- ou \emph{centraux}, abrégés en CPU -- des processeurs graphiques -- dénommés GPU --, qui contiennent beaucoup plus de cœurs mais exécutent obligatoirement tous la même instruction en même temps. Bien sûr les deux types peuvent coexister sur une même machine.

En modifiant la nature du matériel (et non seulement ses capacités d'ordre physique comme la fréquence d'horloge) cela induit des modifications dans la façon de transcrire les algorithmes (par exemple pour utiliser le côté superscalaire), ainsi que dans les algorithmes eux-mêmes qui ne sont parfois plus du tout adaptés (notamment à cause des communications entre les processeurs, inexistantes auparavant). Écrire un programme destiné à une machine parallèle nécessite donc des algorithmes souvent très spécifiques à la configuration de la machine d'exécution, ce qui est d'autant plus complexe avec les architectures des machines actuelles de plus en plus hétérogènes.

\section{\'Ecriture de code parallèle}

Les algorithmes étant souvent spécifiques au type de parallélisme et à l'architecture de la machine cible, les langages permettant d'écrire des programmes parallèles automatisent difficilement la génération de codes parfaitement adaptés à la machine cible. En pratique la plupart des langages modernes permettent l'expression du parallélisme sans le découvrir eux-même ; certains outils aident à écrire du code parallèle mais ils sont souvent expérimentaux et nécessitent alors que le code automatiquement produit soit vérifié avant d'être compilé\footnote{Lors de la transcription d'un problème en informatique, il est d'abord nécessaire de transcrire le problème vers un langage informatique (le code) -- compréhensible par les humains -- qui est alors \emph{compilé} par un programme préexistant (le compilateur) qui le transforme en fichier binaire exécutable -- compréhensible par \emph{la} machine.}.

Les difficultés inhérentes à la production d'un programme parallèle sont de plusieurs natures, notamment l'identification des sections parallèles et de celles obligatoirement séquentielles, ainsi que la gestion des communication et l'utilisation de toutes les ressources disponibles sur la matériel. L'identification des régions parallélisables ou non dépend de l'algorithme utilisé pour résoudre le problème voulu. Un programme bien parallélisé réduit au maximum le nombre d'opérations qui sont exécutées séquentiellement, afin que la majeure partie du temps d'exécution soit passée dans des sections parallèles où toute la puissance de la machine parallèle est utilisée. Par ailleurs il est essentiel de gérer les communications entre les différents processeurs afin que des calculs puissent être effectués pendant les communications ; on parle alors du recouvrement des communications. Enfin il convient d'utiliser le plus de ressources possibles sans que cette utilisation n'entraîne d'effort trop important, par exemple si de nombreuses parties de l'algorithme nécessitent une synchronisation de tous les processeurs -- ce qui est très coûteux en temps.

Actuellement c'est au programmeur qu'incombent presque toutes ces tâches, or cela nécessite beaucoup de compétences, de temps, et de tests avant de parvenir à un résultat acceptable. S'il n'est pas toujours possible de découvrir le parallélisme automatiquement, il n'est pas non plus toujours facile de savoir où le décrire. Alors que certaines options sont très proches du processeur (comme la caractère superscalaire) et doivent être gérées de manière assez fines, d'autres peuvent être prises en charge à un plus haut-niveau comme le lancement de plusieurs procédures distinctes en parallèle (a priori chacune des procédures étant alors associée à un processeur libre). Cela amène à une hiérarchisation du parallélisme, ce qui complique encore plus la tâche du programmeur. Enfin certains paramètres du programme peuvent êtres calculés lors de la compilation -- paramètres statiques -- et d'autres uniquement lors de l'exécution -- paramètres dynamiques. Lorsque le programmeur écrit un code en HPC, il espère donc avoir le plus possible de paramètres statiques, selon les possibilités du compilateur et du langage. 

Une des problématiques du parallélisme est donc de faciliter l'écriture de programme, soit en découvrant automatiquement le parallélisme -- ce qui est plutôt difficile -- , soit en fournissant au programmeur des outils adaptés -- ce qui est un peu plus facile.

\section{Choix du niveau d'expression du parallélisme}

Pour écrire un programme informatique résolvant un problème donné, il est nécessaire de passer par de nombreuses étapes intermédiaires qui sont tout autant de stades où le parallélisme peut être décrit. De très nombreuses possibilités existent et nous ne les présenteront pas en intégralité ici. Le procédé classique consiste à premièrement décrire le problème de manière algorithmique, deuxièmement à le transcrire dans un langage de programmation donnée -- et utiliser les outils natifs du langage ou alors les bibliothèques spécifiques pour le parallélisme -- et troisièmement à compiler le code écrit à l'étape précédente grâce à différents outils internes au compilateur.  

Introduire un nouveau langage dédié au parallélisme est complexe car il faut non seulement développer le compilateur et les bibliothèques standards associés , mais aussi le rendre simple à utiliser et si possible pas trop éloigné des autres paradigmes dominants afin de faciliter son appréhension. D'un autre côté garder les langages existants implique de modifier les compilateurs pour qu'ils découvrent automatiquement les possibilités de parallélisation, ce qui est difficile à mettre en œuvre -- à cause de la complexité même de trouver le parallélisme, et à cause de la complexité des compilateurs modernes. Les deux solutions existent actuellement avec un succès mitigé\footnote{Ce constat est difficile à vérifier, mais à titre d'exemple les langages intrinsèquement parallèles que sont \textsf{\href{http://chapel.cray.com/}{Chapel}} ou \textsf{\href{https://www.cilkplus.org/}{Cilk}} voire \textsf{\href{http://www.erlang.org/}{Erlang}} ne sont pas enseignés dans la filière PRCD de l'Enseirb-Matmeca, probablement car ils ne sont pas suffisamment répandus. Par ailleurs pour les capacités des compilateurs, \textsf{gcc} comprend les directives de la bibliothèque OpenMP, mais ne les trouve pas automatiquement (bien qu'elles soient souvent très simples), en revanche les compilateurs modernes sont souvent capables d'exploiter eux-mêmes la caractère superscalaire d'un processeur.}.

Dans le cadre du stage, nous nous sommes alors plutôt intéressés aux possibilités d'un Domain Specific Embedded Language (DSEL, équivalent de \emph{langage spécifique embarqué} en français) qui permet une charge de développement moindre, ainsi qu'une appréhension rapide. En effet il s'agit alors d'une bibliothèque fournie au programmeur dans un langage courant préexistant, ne comportant que quelques fonctions reprenant uniquement les paradigmes dont le programmeur a besoin pour résoudre un type de problème spécifique, et se basant sur des bibliothèques performantes préexistantes. Le DSEL se différencie d'un DSL (équivalent de \emph{langage spécifique}) par le fait qu'il est disponible au sein d'un langage général, et permet donc potentiellement au développeur d'effectuer des tâches connexes à la résolution du problème dans un seul et même langage. Il peut ainsi utiliser plusieurs DSEL dans le même code, et donc résoudre des problèmes complexes avec un formalisme adapté à chacun d'entre eux (et par là-même avoir de meilleures performances). Le DSEL est ainsi une solution intermédiaire efficace, qui nous a semblé adaptée à l'objectif du stage qui est la parallélisation automatique d'une catégorie spécifique de programmes. Plutôt que de focaliser ce DSEL sur un type spécifique de parallélisme, ou sur un type spécifique d'algorithmes ou paradigmes comme le font de nombreux outils, ce DSEL est focalisé sur une catégorie de problèmes et doit permettre de cacher justement tout ce qui relève du parallélisme.




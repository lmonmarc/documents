%% -*- eval: (flyspell-mode 1); -*-

\chapter{Introduction}
%\addcontentsline{toc}{chapter}{Introduction}

Un des aspects de la recherche informatique consiste à améliorer des programmes afin de rendre leur temps d'exécution plus rapide ; il s'agit du \emph{calcul haute performance} (ou HPC, pour High Performance Computing). 
%Les performances --- en matière de temps d'exécution --- sont primordiales par exemple pour les codes de simulation numérique dédiés à la météorologie qui calculent les prévisions du lendemain, où bien entendu la simulation doit donc s'être terminée en moins d'une nuit. 
Dans ce cadre le stage a porté sur la génération automatique de certains de ces programmes améliorés. 
Les sections suivantes présentent brièvement quelques rappels sur les problématiques HPC, ainsi que l'objectif du stage : l'adaptation automatique d'une certaine catégorie de problèmes au matériel prévu pour le HPC. Le chapitre $2$ précise plus particulièrement ces problématiques et l'objectif tandis que le chapitre $3$ se concentre sur le travail réalisé. Le chapitre $4$ en propose une évaluation et est suivi par la conclusion. 


\section{Parallélisation de programmes}

Diverses techniques existent pour améliorer les performances d'un programme et plus particulièrement de son temps d'exécution. Deux aspects peuvent entrer en compte : les algorithmes utilisés dans le programme ainsi que le matériel qui l'exécute. Le facteur limitant à améliorer en priorité est alors le matériel, car il influe sur la nature des algorithmes pouvant être utilisés.

La solution la plus simple pour améliorer les performances consisterait à se reposer uniquement sur la loi de Moore, c'est-à-dire sur le fait que les capacités du matériel sur lequel le programme est exécuté sont doublées tous les ans. Or, cette loi exponentielle n'est maintenant plus valable puisque la fréquence d'horloge des processeurs modernes stagne depuis quelques années à une valeur proche de $3$ GHz. 
%C'est cette fréquence d'horloge qui impose au matériel (si celui-ci est constitué de cet unique processeur) le nombre d'opérations qu'il va pouvoir effectuer en une seconde. Puisque la fréquence n'augmente plus, le nombre d'opérations par seconde pour ce type de matériel ne peut plus augmenter non plus, aucun gain de rapidité d'exécution n'est alors possible.

%La première solution s'appuyait ainsi sur l'augmentation de la fréquence, c'est-à-dire de la rapidité de chaque opération sachant qu'une seule est faite à la fois. Au lieu d'augmenter la rapidité des opérations, 
Une solution est d'augmenter le nombre d'opérations effectuées \emph{en même temps} par un processeur, ce qui est l'introduction du parallélisme dans le matériel.
%Celui-ci peut s'effectuer de plusieurs manières : à l'intérieur et à l'extérieur du processeur. À l'intérieur,
Il s'agit soit de subdiviser les instructions en  micro-instructions afin de créer un pipeline (ainsi une instruction peut commencer immédiatement après la fin de la première micro-instruction de l'instruction précédente), soit de dupliquer certains composants de l'architecture afin d'exécuter plusieurs instructions sur à la fois, soit enfin l'application de la même instruction sur plusieurs données à la fois. Les processeurs modernes intègrent tous ces trois technologies : le pipeline d'instructions, l'architecture nommée \emph{superscalaire}, et les calculs dits \emph{vectoriels}.
%À l'extérieur il s'agit de connecter plusieurs processeurs les uns avec les autres, tous participant aux calculs nécessités par le programme de base. Différentes manières de relier les processeurs existent, ils peuvent alors être appelés des cœurs ; dans toute la suite du rapport, un processeur est synonyme d'un ensemble de cœurs connectés entre eux par de la mémoire cache hiérarchique en accès uniforme\footnote{Cette notion sera explicitée au point \ref{sec:stencil_base}.}.
Les processeurs peuvent eux-mêmes être répliqués dans leur ensemble, soit sur une même surface de silicium (processeurs multicœurs), soit distinctement et alors reliés par un bus (multiprocesseurs), soit encore par une combinaison de ces deux techniques.
Enfin en dehors des processeurs classiques --- ou \emph{centraux}, abrégés en CPU --- existent aussi des processeurs graphiques --- dénommés GPU ---, qui contiennent beaucoup de cœurs mais exécutent obligatoirement tous la même instruction en même temps. Bien sûr les deux types peuvent coexister sur une même machine.

La nature du matériel (et non seulement ses capacités d'ordre physique comme la fréquence d'horloge) nécessite une conception des algorithmes en adéquation (par exemple pour utiliser le côté superscalaire). 
%cela induit des modifications dans la façon de transcrire les algorithmes (par exemple pour utiliser le côté superscalaire), ainsi que dans les algorithmes eux-mêmes qui ne sont parfois plus du tout adaptés (notamment à cause des communications entre les processeurs, inexistantes auparavant). 
Écrire un programme destiné à une machine parallèle impose donc d'avoir des algorithmes souvent très spécifiques à la configuration de la machine d'exécution, ce qui est d'autant plus complexe avec les architectures des machines actuelles de plus en plus hétérogènes, et peut poser des problèmes de portabilité des applications.

\section{\'Ecriture de code parallèle}

%Les algorithmes étant souvent spécifiques au type de parallélisme et à l'architecture de la machine cible, les langages permettant d'écrire des programmes parallèles automatisent difficilement la génération de codes parfaitement adaptés à la machine cible. En pratique la plupart des langages modernes permettent l'expression du parallélisme sans le découvrir eux-même ; certains outils aident à écrire du code parallèle mais ils sont souvent expérimentaux et nécessitent alors que le code automatiquement produit soit vérifié avant d'être compilé\footnote{Lors de la transcription d'un problème en informatique, il est d'abord nécessaire de transcrire le problème vers un langage informatique (le code) --- compréhensible par les humains --- qui est alors \emph{compilé} par un programme préexistant (le compilateur) qui le transforme en fichier binaire exécutable --- compréhensible par \emph{la} machine.}.

Le procédé classique consiste à premièrement décrire le problème de manière algorithmique, deuxièmement à le transcrire dans un langage de programmation donnée --- et utiliser les outils natifs du langage ou alors les bibliothèques spécifiques pour le parallélisme --- et troisièmement à compiler le code écrit à l'étape précédente grâce à différents outils internes au compilateur. Les deux premières étapes sont réalisées par le développeur, à qui il convient de faciliter la tâche. 

Les difficultés inhérentes à la production d'un programme parallèle sont de plusieurs natures, notamment l'identification des sections parallèles et de celles obligatoirement séquentielles, ainsi que la gestion des communication et l'utilisation de toutes les ressources disponibles sur la matériel. L'identification des régions parallélisables ou non dépend de l'algorithme utilisé pour résoudre le problème voulu. Un programme bien parallélisé réduit au maximum le nombre d'opérations qui sont exécutées séquentiellement, afin que la majeure partie du temps d'exécution soit passée dans des sections parallèles où toute la puissance de la machine parallèle est utilisée. Par ailleurs il est essentiel de gérer les communications entre les différents processeurs afin que des calculs puissent être effectués pendant les communications ; on parle alors du recouvrement des communications. Enfin il convient d'utiliser le plus de ressources possibles sans que cette utilisation n'entraîne d'effort trop important, par exemple si de nombreuses parties de l'algorithme nécessitent une synchronisation de tous les processeurs --- ce qui est très coûteux en temps.

Actuellement c'est au programmeur qu'incombent presque toutes les tâches de parallélisation\footnote{En ce qui concerne les capacités des compilateurs, \textsf{gcc} comprend les directives de la bibliothèque \textsf{OpenMP}, mais ne les trouve pas automatiquement (bien qu'elles soient souvent très simples) et c'est donc au programmeur de les écrire. En revanche les compilateurs modernes sont capables d'exploiter eux-mêmes la caractère superscalaire d'un processeur, mais pas toujours de manière optimale. De même certains calculs vectoriels peuvent être détectés automatiquement, mais ils sont le plus souvent déclenchés via des fonctions spécifiques appelées par le développeur.}. %, or cela nécessite beaucoup de compétences, de temps, et de tests avant de parvenir à un résultat acceptable. S'il n'est pas toujours possible de découvrir le parallélisme automatiquement, il n'est pas non plus toujours facile de savoir où le décrire. Alors que 
Certaines options sont très proches du processeur (comme les capacités vectorielles) et doivent être gérées de manière assez fines, d'autres peuvent être prises en charge à un plus haut-niveau comme le lancement de plusieurs procédures distinctes en parallèle (a priori chacune des procédures étant alors associée à un processeur libre). %Cela amène à une hiérarchisation du parallélisme, ce qui complique encore plus la tâche du programmeur.
Par ailleurs certains paramètres du programme peuvent être calculés lors de la compilation --- paramètres statiques --- et d'autres uniquement lors de l'exécution --- paramètres dynamiques. Or, pour améliorer la rapidité du programme, il est souhaitable de limiter les calculs lors de l'exécution. Lorsque le programmeur écrit un code en HPC, il espère donc avoir le plus possible de paramètres statiques, selon les possibilités du compilateur et du langage. 

Une des problématiques du parallélisme est donc de faciliter l'écriture de programme, soit en découvrant automatiquement le parallélisme --- ce qui est plutôt difficile --- , soit en fournissant au programmeur des outils adaptés --- ce qui est un peu plus facile.

\section{Choix du niveau d'expression du parallélisme}

%Pour écrire un programme informatique résolvant un problème donné, il est nécessaire de passer par de nombreuses étapes intermédiaires qui sont tout autant de stades où le parallélisme peut être décrit. De très nombreuses possibilités existent et nous ne les présenteront pas en intégralité ici. 

Compte-tenu de la complexité de l'écriture de code parallèle, des outils spécifiques ont été mis en place afin de faciliter le développement de tels programmes. Or, ces outils --- langages, bibliothèques, ou compilateurs --- sont rares et souvent trop spécifiques ou au contraire trop généraux. L'objectif du stage est donc de développer un nouvel outil adapté à un certain type de problèmes pour lequel peu d'outils existent et ne sont pas adaptés à la configuration hétérogène des machines actuelles.

Introduire un nouveau langage dédié au parallélisme est complexe car il faut non seulement développer le compilateur et les bibliothèques standards associés , mais aussi le rendre simple à utiliser et si possible pas trop éloigné des autres paradigmes dominants afin de faciliter son appréhension. D'un autre côté garder les langages existants implique de modifier les compilateurs pour qu'ils découvrent automatiquement les possibilités de parallélisation, ce qui est difficile à mettre en œuvre --- à cause de la complexité même de trouver le parallélisme, et à cause de la complexité des compilateurs modernes.
%Les deux solutions existent actuellement avec un succès mitigé\footnote{Ce constat est difficile à vérifier, mais à titre d'exemple les langages intrinsèquement parallèles que sont \textsf{Chapel} ou \textsf{Cilk} voire \textsf{Erlang} ne sont pas enseignés dans la filière PRCD de l'Enseirb-Matmeca, probablement car ils ne sont pas suffisamment répandus. Par ailleurs pour les capacités des compilateurs, \textsf{gcc} comprend les directives de la bibliothèque \textsf{OpenMP}, mais ne les trouve pas automatiquement (bien qu'elles soient souvent très simples), en revanche les compilateurs modernes sont souvent capables d'exploiter eux-mêmes la caractère superscalaire d'un processeur.}.

%Dans le cadre du stage, nous nous sommes alors plutôt intéressés aux possibilités d'un Domain Specific Embedded Language (DSEL, équivalent de \emph{langage spécifique embarqué} en français) qui permet une charge de développement moindre, ainsi qu'une appréhension rapide. En effet il s'agit alors d'une bibliothèque fournie au programmeur dans un langage courant préexistant, ne comportant que quelques fonctions reprenant uniquement les paradigmes dont le programmeur a besoin pour résoudre un type de problème spécifique, et se basant sur des bibliothèques performantes préexistantes. 
Nous avons donc décidé de développer une approche s'appuyant sur un Domain Specific Embedded Language (DSEL, équivalent de \emph{langage spécifique embarqué} en français) sous la forme d'une bibliothèque, avec comme objectifs de faciliter la prise en main du programmeur et de réduire sa charge de développement, tout en préservant ou en améliorant les performances de ses programmes.
% à recaser plus tard
%Le DSEL se différencie d'un DSL (équivalent de \emph{langage spécifique}) par le fait qu'il est disponible au sein d'un langage général, et permet donc potentiellement au développeur d'effectuer des tâches connexes à la résolution du problème dans un seul et même langage. Il peut ainsi utiliser plusieurs DSEL dans le même code, et donc résoudre des problèmes complexes avec un formalisme adapté à chacun d'entre eux (et par là-même avoir de meilleures performances). Le DSEL est ainsi une solution intermédiaire efficace, qui nous a semblé adaptée à l'objectif du stage qui est la parallélisation automatique d'une catégorie spécifique de programmes. 
Plutôt que de focaliser ce DSEL sur un type spécifique de parallélisme, ou sur un paradigme comme le font de nombreux outils, ce DSEL est focalisé sur une catégorie de problèmes et doit permettre de cacher justement tout ce qui relève du parallélisme.



